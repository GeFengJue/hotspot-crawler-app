name: Deploy to GitHub Pages

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
  # 在定时爬虫任务完成后触发部署
  workflow_run:
    workflows: ["Scheduled Data Crawling"]
    types: [completed]
    branches: [main]

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  deploy:
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: pip install -r requirements.txt
      
    - name: Build static files
      run: |
        # 复制必要的文件到部署目录
        mkdir -p dist
        cp index.html dist/
        # 确保hotspot_data.json文件存在
        if [ -f hotspot_data.json ]; then
          cp hotspot_data.json dist/
        else
          # 如果不存在，尝试复制最新的数据文件
          latest_json=$(ls -t hotspot_data_*.json 2>/dev/null | head -1)
          if [ -n "$latest_json" ]; then
            cp "$latest_json" dist/hotspot_data.json
          else
            echo "警告：未找到数据文件，将创建空文件"
            echo '{"热点资讯": [], "今日热点": [], "财经日历": [], "公社热帖": []}' > dist/hotspot_data.json
          fi
        fi
        cp -r functions/ dist/functions/ || true
        cp _redirects dist/ || true
        
    - name: Setup Pages
      uses: actions/configure-pages@v4
      
    - name: Upload artifact
      uses: actions/upload-pages-artifact@v4
      with:
        path: 'dist'
        
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4